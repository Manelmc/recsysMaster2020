{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3: Collaborative-based recommender systems: Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(150000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 150 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssegui/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:12: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/Users/ssegui/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:15: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/Users/ssegui/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:20: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La BD has 1000209 ratings\n",
      "La BD has  6040  users\n",
      "La BD has  3706  movies\n",
      "(797758, 8)\n",
      "(202451, 8)\n",
      "Int64Index([], dtype='int64')\n",
      "Training data_set has 797758 ratings\n",
      "Test data set has 202451 ratings\n",
      "La BD has  3706  movies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#NETFLIX REAL 50.000.000 usuaris and 100.000 items\n",
    "%autosave 150\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Load Data set\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('ml-1m/users.dat', sep='::', names=u_cols)\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', names=r_cols)\n",
    "\n",
    "# the movies file contains columns indicating the movie's genres\n",
    "# let's only load the first three columns of the file with usecols\n",
    "m_cols = ['movie_id', 'title', 'release_date']\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep='::', names=m_cols, usecols=range(3), encoding='latin-1')\n",
    "\n",
    "# ConstrucciÃ³ del DataFrame\n",
    "data = pd.merge(pd.merge(ratings, users), movies)\n",
    "data = data[['user_id','title', 'movie_id','rating','release_date','sex','age']]\n",
    "\n",
    "\n",
    "print(\"La BD has \"+ str(data.shape[0]) +\" ratings\")\n",
    "print(\"La BD has \", data.user_id.nunique(),\" users\")\n",
    "print(\"La BD has \", data.movie_id.nunique(), \" movies\")\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(estimate_f,data_train,data_test):\n",
    "    \"\"\" RMSE-based predictive performance evaluation with pandas. \"\"\"\n",
    "    ids_to_estimate = zip(data_test.user_id, data_test.movie_id)\n",
    "    estimated = np.array([estimate_f(u,i) if u in data_train.user_id else 3 for (u,i) in ids_to_estimate ])\n",
    "    real = data_test.rating.values\n",
    "    return compute_rmse(estimated, real)\n",
    "\n",
    "def compute_rmse(y_pred, y_true):\n",
    "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
    "\n",
    "\n",
    "## Divide the data in two sets: training and test\n",
    "def assign_to_set(df):\n",
    "    sampled_ids = np.random.choice(df.index,\n",
    "                                   size=np.int64(np.ceil(df.index.size * 0.2)),\n",
    "                                   replace=False)\n",
    "    df.loc[sampled_ids, 'for_testing'] = True\n",
    "    return df\n",
    "\n",
    "data['for_testing'] = False\n",
    "grouped = data.groupby('user_id', group_keys=False).apply(assign_to_set)\n",
    "data_train = data[grouped.for_testing == False]\n",
    "data_test = data[grouped.for_testing == True]\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "print(data_train.index & data_test.index)\n",
    "\n",
    "print(\"Training data_set has \"+ str(data_train.shape[0]) +\" ratings\")\n",
    "print(\"Test data set has \"+ str(data_test.shape[0]) +\" ratings\")\n",
    "print(\"La BD has \", data.movie_id.nunique(), \" movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Matrix as Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "class SVD_CollaborativeFiltering:\n",
    "    \"\"\" Collaborative filtering using a custom sim(u,u'). \"\"\"\n",
    "    \n",
    "    def __init__(self,DataFrame, num_components=10):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.df=DataFrame\n",
    "        self.sim = pd.DataFrame(np.sum([0]),columns=data_train.user_id.unique(), index=data_train.user_id.unique())\n",
    "        self.num_components=num_components\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\" Prepare data structures for estimation. Similarity matrix for users \"\"\"\n",
    "        allUsers=set(self.df['user_id'])\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=self.num_components)\n",
    "        urm = pd.pivot_table(self.df[['user_id','movie_id','rating']],columns='user_id',index='movie_id',values='rating',fill_value=0)\n",
    "        X= np.float32(urm.values)\n",
    "        \n",
    "        #Lets continue.....\n",
    "                \n",
    "    def estimate(self, user_id, movie_id):\n",
    "        movie_users = self.df[self.df['movie_id'] ==movie_id]\n",
    "        #movie_users = movie_users.set_index('user_id')\n",
    "        \n",
    "        allUsers = movie_users.user_id\n",
    "        \n",
    "        a = movie_users.rating.values\n",
    "        b = reco.sim[user_id-1][allUsers-1] \n",
    "        rating_num = np.sum(a*b)\n",
    "        rating_den = np.sum(b)\n",
    "        if rating_den==0: \n",
    "            if self.df.rating[self.df['movie_id']==movie_id].mean()>0:\n",
    "                # return the mean movie rating if there is no similar for the computation\n",
    "                return self.df.rating[self.df['movie_id']==movie_id].mean()\n",
    "            else:\n",
    "                # else return mean user rating \n",
    "                return self.df.rating[self.df['user_id']==user_id].mean()\n",
    "        return rating_num/rating_den\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3244454192889794"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reco = SVD_CollaborativeFiltering(data_train,num_components=40)\n",
    "reco.learn()\n",
    "reco.estimate(user_id=2,movie_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('RMSE for Collaborative Recomender: %s' % evaluate(reco.estimate,data_train,data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FACTORIZATION MODEL SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "class SVD_CollaborativeFiltering:\n",
    "    \"\"\" Collaborative filtering using a custom sim(u,u'). \"\"\"\n",
    "    \n",
    "    def __init__(self,DataFrame, num_components=10,\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.df = DataFrame\n",
    "        self.num_components = num_components\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        \n",
    "        urm = pd.pivot_table(self.df[['user_id','movie_id','rating']],columns='movie_id',index='user_id',values='rating',fill_value=0)\n",
    "        self.n_users, self.n_items = urm.shape\n",
    "        self.ratings = np.float32(urm.values)\n",
    "        \n",
    "        user_index = np.arange(len(urm.index))\n",
    "        self.users = dict(zip(user_index,urm.index ))\n",
    "        self.users_index2id = dict(zip(urm.index,user_index)) \n",
    "        \n",
    "        movie_index = np.arange(len(urm.columns))\n",
    "        self.movies = dict(zip(movie_index,urm.columns ))   \n",
    "        self.movies_index2id = dict(zip(urm.columns, movie_index))\n",
    "\n",
    "        self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "        self.n_samples = len(self.sample_row)\n",
    "    \n",
    "    def __sdg__(self):\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            user_id = self.users[u]\n",
    "            item_id = self.movies[i]\n",
    "            \n",
    "            prediction = self.estimate(user_id, item_id)\n",
    "            error = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs[u, :] += self.learning_rate * \\\n",
    "                                    (error * self.item_vecs[i, :] - \\\n",
    "                                     self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "                                    (error * self.user_vecs[u, :] - \\\n",
    "                                     self.item_fact_reg * self.item_vecs[i,:])\n",
    "                \n",
    "                \n",
    "    def learn(self,n_iter = 10, learning_rate=0.001):\n",
    "        \"\"\" Train the model. \"\"\"\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        \n",
    "        # initialize latent vectors\n",
    "        self.user_vecs = np.random.normal(scale=1./self.num_components,\\\n",
    "                                          size=(self.n_users, self.num_components))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.num_components,\n",
    "                                          size=(self.n_items, self.num_components))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        ctr =1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 1 == 0:\n",
    "                print('Iteration: {}'.format(ctr))\n",
    "            self.training_indices = np.arange(self.n_samples)\n",
    "            #shuffle training samples\n",
    "            np.random.shuffle(self.training_indices)\n",
    "            self.__sdg__()\n",
    "            ctr += 1\n",
    "                \n",
    "            print('\\tTrain mse: %s' % evaluate(reco.estimate,data_train,data_train))\n",
    "            print('\\tTest mse: %s' % evaluate(reco.estimate,data_test,data_train))\n",
    "    \n",
    "                \n",
    "    def estimate(self, user_id, movie_id):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        u = self.users_index2id[user_id]\n",
    "        i = self.movies_index2id[movie_id]\n",
    "        prediction =  self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reco = SVD_CollaborativeFiltering(data_train,num_components=40)\n",
    "reco.learn(n_iter = 50)\n",
    "reco.estimate(user_id=2,movie_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
